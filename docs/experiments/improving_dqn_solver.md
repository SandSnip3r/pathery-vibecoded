# Improving the DQN Genetic Solver

**Status: Closed**

## Objective

The primary goal is to improve the performance of the `dqn_genetic_solver` to a point where it consistently outperforms the baseline `genetic_solver`. This document outlines a structured, iterative plan to achieve this while managing the complexity of multiple changing variables (code, hyperparameters, and training data).

## Problem Analysis

The core issue is likely a **distribution shift**. The DQN model was pre-trained on data generated by an older, purely random `genetic_solver`. Since then, the codebase and hyperparameters have evolved. When the pre-trained model is used in the `dqn_genetic_solver`, it's operating in an environment (a data distribution) different from the one it was trained on. Its learned policy may no longer be optimal, leading to suboptimal performance.

The current approach of using the model for purely exploitative mutations (`epsilon=0.0`) also prevents it from discovering new, potentially better strategies in the context of the updated solver.

## Action Plan

We will take a series of deliberate, measurable steps. Each step is designed to be a relatively short experiment, allowing us to gauge progress without committing to long-running training sessions.

### Step 1: Establish a Reliable Performance Baseline

Before we can improve the solver, we need to know exactly how the current versions perform. This will serve as our benchmark.

**Action:**
1.  Create a standardized benchmark suite of 10-15 puzzles from `data/puzzles/ucu/`. Select a range of difficulties (e.g., some easy, some medium, some hard).
2.  Use the `scripts/benchmark.py` script to run the baseline `genetic_solver` against this puzzle suite.
3.  Record the average score, solution rate, and average time per puzzle. This is the target to beat.

### Step 2: Quantify the Current DQN Solver's Performance

Run the same benchmark on the `dqn_genetic_solver` with the existing pre-trained model.

**Action:**
1.  Using the same benchmark suite, run the `dqn_genetic_solver`.
2.  Record its performance metrics.
3.  This will give us a clear, quantitative measure of the performance gap between the baseline and the current DQN-guided solver.

### Step 3: Fine-Tuning the DQN Model

Instead of retraining from scratch, we will fine-tune the existing model on a small, high-quality dataset generated by the *current* solver. This is much more time-efficient.

**Action:**
1.  **Introduce Exploration:** In `src/pathery/solvers/dqn_genetic_solver.py`, modify the `_mutate` method to incorporate an `epsilon` > 0. This will allow the agent to explore new mutations instead of only exploiting its current knowledge. A good starting point would be to add `epsilon` as a parameter to the solver's `__init__` and use it in the `agent.choose_action` call. An initial value of `epsilon=0.15` is recommended.
2.  **Collect New Data:** Run a *short* data collection session (e.g., 1-2 hours) using `scripts/run_data_collection.py`. Ensure it is configured to use the `dqn_genetic_solver` with the new exploration capability. This will generate a fresh dataset that is highly relevant to the solver's current state.
3.  **Fine-Tune:** Run `scripts/train_dqn.py` on this new, smaller dataset for a limited number of epochs. This will update the model's weights to better fit the current data distribution without starting from zero.

### Step 4: Evaluate the Fine-Tuned Solver

After fine-tuning, repeat the benchmark from Step 2.

**Action:**
1.  Run the benchmark suite with the fine-tuned `dqn_genetic_solver`.
2.  Compare the results to the baseline (Step 1) and the pre-tuned DQN solver (Step 2). This will clearly show whether the fine-tuning process was successful.

### Step 5: Systematic Hyperparameter Tuning (If Necessary)

If performance is still not satisfactory, the next logical step is to tune the genetic algorithm's hyperparameters. A systematic approach is better than manual tweaking.

**Action:**
1.  Create a new script, e.g., `scripts/tune_ga.py`, dedicated to hyperparameter optimization.
2.  This script should run the `dqn_genetic_solver` on a *single, representative puzzle* for a *fixed, short duration* (e.g., 200 generations).
3.  Use a search strategy (like Grid Search or Random Search) to explore different combinations of key hyperparameters:
    *   `population_size`
    *   `mutation_rate`
    *   `crossover_rate`
    *   `tournament_size`
    *   `epsilon` (for the DQN solver)
4.  The goal is to quickly identify the most promising hyperparameter sets for more extensive testing.

By following this iterative plan, we can systematically address the moving parts and work towards a more powerful solver in a controlled and time-efficient manner.

## Conclusion

The fine-tuned model performed significantly worse than the baseline, indicating a failure in the learning process. The reward function, while technically correct, provided a poor learning signal, causing the model to become overly conservative. This experiment is now closed, and a new experiment with a reward shaping approach will be initiated.
